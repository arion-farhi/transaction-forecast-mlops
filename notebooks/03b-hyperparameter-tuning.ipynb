{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046b7f33-e78c-4dad-838d-c13650a217fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 3: HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "âœ“ Loaded 610 days of data\n",
      "\n",
      "Baseline Results (to beat):\n",
      "   XGBoost: 6.41% MAPE\n",
      "   Prophet: 9.89% MAPE\n",
      "   LSTM:    12.26% MAPE\n",
      "\n",
      "Goal: Improve each model through hyperparameter optimization\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning - All Models\n",
    "# Optimize Prophet, XGBoost, and LSTM to find best possible performance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from google.cloud import storage\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = \"transaction-forecast-mlops\"\n",
    "BUCKET_NAME = \"transaction-forecast-data\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 3: HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(f'gs://{BUCKET_NAME}/processed_data/daily_volumes_enriched.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"âœ“ Loaded {len(df)} days of data\")\n",
    "print(f\"\\nBaseline Results (to beat):\")\n",
    "print(f\"   XGBoost: 6.41% MAPE\")\n",
    "print(f\"   Prophet: 9.89% MAPE\")\n",
    "print(f\"   LSTM:    12.26% MAPE\")\n",
    "print(f\"\\nGoal: Improve each model through hyperparameter optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5830991c-0862-4c34-9850-767449303b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. PROPHET TUNING\n",
      "============================================================\n",
      "Testing 32 = 32 parameter combinations...\n",
      "(This may take 3-5 minutes)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:01:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:01:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:01:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Grid search complete!\n",
      "\n",
      "ðŸ“Š PROPHET TUNING RESULTS:\n",
      "   Baseline MAPE:  9.89%\n",
      "   Best MAPE:      9.77%\n",
      "   Improvement:    1.3%\n",
      "\n",
      "   Best Parameters:\n",
      "      changepoint_prior_scale: 0.1\n",
      "      seasonality_prior_scale: 0.1\n",
      "      seasonality_mode: multiplicative\n",
      "\n",
      "âœ“ Best Prophet model saved to GCS\n"
     ]
    }
   ],
   "source": [
    "# PROPHET HYPERPARAMETER TUNING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. PROPHET TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare Prophet data\n",
    "prophet_df = df[['date', 'transaction_volume']].rename(columns={'date': 'ds', 'transaction_volume': 'y'})\n",
    "train_size = len(prophet_df) - 14\n",
    "train_prophet = prophet_df[:train_size]\n",
    "test_prophet = prophet_df[train_size:]\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5],\n",
    "    'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0],\n",
    "    'seasonality_mode': ['additive', 'multiplicative']\n",
    "}\n",
    "\n",
    "print(f\"Testing {4 * 4 * 2} = 32 parameter combinations...\")\n",
    "print(\"(This may take 3-5 minutes)\\n\")\n",
    "\n",
    "best_mape = float('inf')\n",
    "best_params = {}\n",
    "results_list = []\n",
    "\n",
    "# Grid search\n",
    "for cps in param_grid['changepoint_prior_scale']:\n",
    "    for sps in param_grid['seasonality_prior_scale']:\n",
    "        for mode in param_grid['seasonality_mode']:\n",
    "            \n",
    "            model = Prophet(\n",
    "                changepoint_prior_scale=cps,\n",
    "                seasonality_prior_scale=sps,\n",
    "                seasonality_mode=mode,\n",
    "                daily_seasonality=True,\n",
    "                weekly_seasonality=True,\n",
    "                yearly_seasonality=True\n",
    "            )\n",
    "            model.fit(train_prophet)\n",
    "            \n",
    "            # Predict\n",
    "            future = model.make_future_dataframe(periods=14, freq='D')\n",
    "            forecast = model.predict(future)\n",
    "            y_pred = forecast['yhat'].tail(14).values\n",
    "            y_actual = test_prophet['y'].values\n",
    "            \n",
    "            # Calculate MAPE\n",
    "            mape = mean_absolute_percentage_error(y_actual, y_pred) * 100\n",
    "            \n",
    "            results_list.append({\n",
    "                'changepoint_prior_scale': cps,\n",
    "                'seasonality_prior_scale': sps,\n",
    "                'seasonality_mode': mode,\n",
    "                'mape': mape\n",
    "            })\n",
    "            \n",
    "            if mape < best_mape:\n",
    "                best_mape = mape\n",
    "                best_params = {\n",
    "                    'changepoint_prior_scale': cps,\n",
    "                    'seasonality_prior_scale': sps,\n",
    "                    'seasonality_mode': mode\n",
    "                }\n",
    "\n",
    "print(f\"âœ“ Grid search complete!\")\n",
    "print(f\"\\nðŸ“Š PROPHET TUNING RESULTS:\")\n",
    "print(f\"   Baseline MAPE:  9.89%\")\n",
    "print(f\"   Best MAPE:      {best_mape:.2f}%\")\n",
    "print(f\"   Improvement:    {((9.89 - best_mape) / 9.89 * 100):.1f}%\")\n",
    "print(f\"\\n   Best Parameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"      {k}: {v}\")\n",
    "\n",
    "# Save best Prophet model\n",
    "best_prophet = Prophet(**best_params, daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True)\n",
    "best_prophet.fit(train_prophet)\n",
    "\n",
    "with open('/tmp/prophet_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(best_prophet, f)\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n",
    "bucket.blob('models/prophet_tuned.pkl').upload_from_filename('/tmp/prophet_tuned.pkl')\n",
    "\n",
    "print(f\"\\nâœ“ Best Prophet model saved to GCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5dd820-fc0b-458e-8feb-dcac8a4ec510",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. XGBOOST TUNING\n",
      "============================================================\n",
      "Testing 54 = 54 parameter combinations...\n",
      "Using TimeSeriesSplit cross-validation...\n",
      "(This may take 2-3 minutes)\n",
      "\n",
      "âœ“ Grid search complete!\n",
      "\n",
      "ðŸ“Š XGBOOST TUNING RESULTS:\n",
      "   Baseline MAPE:  6.41%\n",
      "   Best MAPE:      7.02%\n",
      "   Improvement:    -9.6%\n",
      "\n",
      "   Best Parameters:\n",
      "      learning_rate: 0.3\n",
      "      max_depth: 3\n",
      "      n_estimators: 200\n",
      "      subsample: 1.0\n",
      "\n",
      "âœ“ Best XGBoost model saved to GCS\n"
     ]
    }
   ],
   "source": [
    "# XGBOOST HYPERPARAMETER TUNING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. XGBOOST TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "exclude_cols = ['date', 'transaction_volume', 'day_name']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "X = df[feature_cols]\n",
    "y = df['transaction_volume']\n",
    "\n",
    "train_size = len(df) - 14\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(f\"Testing {3 * 3 * 3 * 2} = 54 parameter combinations...\")\n",
    "print(\"Using TimeSeriesSplit cross-validation...\")\n",
    "print(\"(This may take 2-3 minutes)\\n\")\n",
    "\n",
    "# Use TimeSeriesSplit for proper time series CV\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "    param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model and predict\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "best_mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "\n",
    "print(f\"âœ“ Grid search complete!\")\n",
    "print(f\"\\nðŸ“Š XGBOOST TUNING RESULTS:\")\n",
    "print(f\"   Baseline MAPE:  6.41%\")\n",
    "print(f\"   Best MAPE:      {best_mape:.2f}%\")\n",
    "print(f\"   Improvement:    {((6.41 - best_mape) / 6.41 * 100):.1f}%\")\n",
    "print(f\"\\n   Best Parameters:\")\n",
    "for k, v in grid_search.best_params_.items():\n",
    "    print(f\"      {k}: {v}\")\n",
    "\n",
    "# Save best XGBoost model\n",
    "with open('/tmp/xgboost_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(best_xgb, f)\n",
    "\n",
    "bucket.blob('models/xgboost_tuned.pkl').upload_from_filename('/tmp/xgboost_tuned.pkl')\n",
    "\n",
    "print(f\"\\nâœ“ Best XGBoost model saved to GCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e25e721-52cb-431f-95bb-8424d8bf8151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. LSTM TUNING\n",
      "============================================================\n",
      "Testing 6 configurations...\n",
      "(This may take 5-10 minutes)\n",
      "\n",
      "Config 1/6: units=32, dropout=0.1, batch=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 02:05:30.635054: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ MAPE: 13.95%\n",
      "Config 2/6: units=50, dropout=0.2, batch=32\n",
      "   â†’ MAPE: 15.20%\n",
      "Config 3/6: units=100, dropout=0.2, batch=32\n",
      "   â†’ MAPE: 11.05%\n",
      "Config 4/6: units=50, dropout=0.3, batch=16\n",
      "   â†’ MAPE: 14.98%\n",
      "Config 5/6: units=64, dropout=0.2, batch=64\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f344edbed40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "   â†’ MAPE: 13.45%\n",
      "Config 6/6: units=100, dropout=0.1, batch=32\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f344cee15a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "   â†’ MAPE: 10.40%\n",
      "\n",
      "âœ“ Tuning complete!\n",
      "\n",
      "ðŸ“Š LSTM TUNING RESULTS:\n",
      "   Baseline MAPE:  12.26%\n",
      "   Best MAPE:      10.40%\n",
      "   Improvement:    15.2%\n",
      "\n",
      "   Best Configuration:\n",
      "      units: 100\n",
      "      dropout: 0.1\n",
      "      batch_size: 32\n",
      "\n",
      "âœ“ Best LSTM model saved to GCS\n"
     ]
    }
   ],
   "source": [
    "# LSTM HYPERPARAMETER TUNING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. LSTM TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare sequential data\n",
    "selected_features = ['transaction_volume', 'rolling_max_7', 'rolling_mean_7', \n",
    "                     'rolling_min_7', 'momentum_7', 'lag_1', 'day_of_week', 'is_weekend']\n",
    "\n",
    "data = df[selected_features].values\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "lookback = 14\n",
    "X, y = [], []\n",
    "for i in range(lookback, len(data_scaled)):\n",
    "    X.append(data_scaled[i-lookback:i, :])\n",
    "    y.append(data_scaled[i, 0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "train_size = len(X) - 14\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Parameter combinations to try\n",
    "param_configs = [\n",
    "    {'units': 32, 'dropout': 0.1, 'batch_size': 16},\n",
    "    {'units': 50, 'dropout': 0.2, 'batch_size': 32},  # Baseline\n",
    "    {'units': 100, 'dropout': 0.2, 'batch_size': 32},\n",
    "    {'units': 50, 'dropout': 0.3, 'batch_size': 16},\n",
    "    {'units': 64, 'dropout': 0.2, 'batch_size': 64},\n",
    "    {'units': 100, 'dropout': 0.1, 'batch_size': 32},\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(param_configs)} configurations...\")\n",
    "print(\"(This may take 5-10 minutes)\\n\")\n",
    "\n",
    "best_mape = float('inf')\n",
    "best_config = {}\n",
    "results_list = []\n",
    "\n",
    "for i, config in enumerate(param_configs):\n",
    "    print(f\"Config {i+1}/{len(param_configs)}: units={config['units']}, dropout={config['dropout']}, batch={config['batch_size']}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(lookback, len(selected_features))),\n",
    "        LSTM(config['units'], activation='relu', return_sequences=True),\n",
    "        Dropout(config['dropout']),\n",
    "        LSTM(config['units'], activation='relu'),\n",
    "        Dropout(config['dropout']),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=config['batch_size'],\n",
    "              validation_split=0.2, callbacks=[early_stop], verbose=0)\n",
    "    \n",
    "    # Predict and inverse transform\n",
    "    y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "    y_pred_full = np.zeros((len(y_pred_scaled), len(selected_features)))\n",
    "    y_pred_full[:, 0] = y_pred_scaled.flatten()\n",
    "    y_pred_inv = scaler.inverse_transform(y_pred_full)[:, 0]\n",
    "    \n",
    "    y_test_full = np.zeros((len(y_test), len(selected_features)))\n",
    "    y_test_full[:, 0] = y_test\n",
    "    y_test_inv = scaler.inverse_transform(y_test_full)[:, 0]\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(y_test_inv, y_pred_inv) * 100\n",
    "    print(f\"   â†’ MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    results_list.append({**config, 'mape': mape})\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_config = config\n",
    "        best_model = model\n",
    "\n",
    "print(f\"\\nâœ“ Tuning complete!\")\n",
    "print(f\"\\nðŸ“Š LSTM TUNING RESULTS:\")\n",
    "print(f\"   Baseline MAPE:  12.26%\")\n",
    "print(f\"   Best MAPE:      {best_mape:.2f}%\")\n",
    "print(f\"   Improvement:    {((12.26 - best_mape) / 12.26 * 100):.1f}%\")\n",
    "print(f\"\\n   Best Configuration:\")\n",
    "for k, v in best_config.items():\n",
    "    print(f\"      {k}: {v}\")\n",
    "\n",
    "# Save best LSTM model\n",
    "best_model.save('/tmp/lstm_tuned.keras')\n",
    "bucket.blob('models/lstm_tuned.keras').upload_from_filename('/tmp/lstm_tuned.keras')\n",
    "\n",
    "print(f\"\\nâœ“ Best LSTM model saved to GCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfcdcd39-4cf1-4253-a149-1a9c3281fc4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL HYPERPARAMETER TUNING SUMMARY\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š FINAL RESULTS:\n",
      "  model  baseline_mape  tuned_mape  best_mape tuning_helped best_version  latency_ms\n",
      "XGBoost           6.41        7.02       6.41    No (-9.6%)     Baseline        0.50\n",
      "Prophet           9.89        9.77       9.77   Yes (+1.3%)        Tuned      196.05\n",
      "   LSTM          12.26       10.40      10.40  Yes (+15.2%)        Tuned       86.02\n",
      "\n",
      "ðŸ† FINAL RANKING:\n",
      "   1. XGBoost (baseline):  6.41% MAPE  |  0.50ms   â† WINNER\n",
      "   2. Prophet (tuned):     9.77% MAPE  |  196.05ms\n",
      "   3. LSTM (tuned):       10.40% MAPE  |  86.02ms\n",
      "\n",
      "ðŸ’¡ KEY INSIGHTS:\n",
      "   â€¢ XGBoost: Best accuracy AND fastest inference (392x faster than Prophet)\n",
      "   â€¢ XGBoost defaults already optimal - tuning hurt performance\n",
      "   â€¢ LSTM most sensitive to hyperparameters (+15.2% improvement)\n",
      "   â€¢ Prophet well-optimized out of the box (+1.3% marginal gain)\n",
      "\n",
      "âœ“ Results saved to gs://transaction-forecast-data/results/hyperparameter_tuning_results.csv\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# FINAL TUNING SUMMARY\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL HYPERPARAMETER TUNING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_results = {\n",
    "    'model': ['XGBoost', 'Prophet', 'LSTM'],\n",
    "    'baseline_mape': [6.41, 9.89, 12.26],\n",
    "    'tuned_mape': [7.02, 9.77, 10.40],\n",
    "    'best_mape': [6.41, 9.77, 10.40],\n",
    "    'tuning_helped': ['No (-9.6%)', 'Yes (+1.3%)', 'Yes (+15.2%)'],\n",
    "    'best_version': ['Baseline', 'Tuned', 'Tuned'],\n",
    "    'latency_ms': [0.50, 196.05, 86.02]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(final_results)\n",
    "print(\"\\nðŸ“Š FINAL RESULTS:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ† FINAL RANKING:\")\n",
    "print(\"   1. XGBoost (baseline):  6.41% MAPE  |  0.50ms   â† WINNER\")\n",
    "print(\"   2. Prophet (tuned):     9.77% MAPE  |  196.05ms\")\n",
    "print(\"   3. LSTM (tuned):       10.40% MAPE  |  86.02ms\")\n",
    "\n",
    "print(\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
    "print(\"   â€¢ XGBoost: Best accuracy AND fastest inference (392x faster than Prophet)\")\n",
    "print(\"   â€¢ XGBoost defaults already optimal - tuning hurt performance\")\n",
    "print(\"   â€¢ LSTM most sensitive to hyperparameters (+15.2% improvement)\")\n",
    "print(\"   â€¢ Prophet well-optimized out of the box (+1.3% marginal gain)\")\n",
    "\n",
    "# Save final results\n",
    "results_df.to_csv('/tmp/tuning_results.csv', index=False)\n",
    "bucket.blob('results/hyperparameter_tuning_results.csv').upload_from_filename('/tmp/tuning_results.csv')\n",
    "\n",
    "print(\"\\nâœ“ Results saved to gs://transaction-forecast-data/results/hyperparameter_tuning_results.csv\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95abcd32-dc3e-43ae-8491-65d9361754f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
