{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b89b20a-b013-4aa5-b756-f696971618c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "Loading clean transaction data...\n",
      "✓ Loaded 610 days\n",
      "Date range: 2016-09-04 00:00:00 to 2018-08-22 00:00:00\n",
      "\n",
      "Current columns: ['date', 'transaction_volume']\n",
      "\n",
      "Starting feature engineering...\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering for Transaction Volume Forecasting\n",
    "# Add features to improve model performance\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import storage\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = \"transaction-forecast-mlops\"\n",
    "BUCKET_NAME = \"transaction-forecast-data\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load clean data\n",
    "print(\"\\nLoading clean transaction data...\")\n",
    "df = pd.read_csv(f'gs://{BUCKET_NAME}/processed_data/daily_volumes_clean.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"✓ Loaded {len(df)} days\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nCurrent columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nStarting feature engineering...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9251aeba-1334-471f-b6f4-f01858c0d003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Adding temporal features...\n",
      "✓ Added 9 temporal features\n",
      "  - day_of_week, day_name, month, quarter\n",
      "  - is_weekend, is_month_start, is_month_end\n",
      "  - day_of_month, week_of_year\n"
     ]
    }
   ],
   "source": [
    "# 1. TEMPORAL FEATURES\n",
    "print(\"\\n[1/5] Adding temporal features...\")\n",
    "\n",
    "df['day_of_week'] = df['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['day_name'] = df['date'].dt.day_name()\n",
    "df['month'] = df['date'].dt.month\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['day_of_month'] = df['date'].dt.day\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "\n",
    "print(f\"✓ Added {9} temporal features\")\n",
    "print(f\"  - day_of_week, day_name, month, quarter\")\n",
    "print(f\"  - is_weekend, is_month_start, is_month_end\")\n",
    "print(f\"  - day_of_month, week_of_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793f180b-4afb-4d0d-993e-f851043edbde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] Adding lag features...\n",
      "✓ Added 4 lag features\n",
      "  - lag_1 (yesterday)\n",
      "  - lag_7 (last week)\n",
      "  - lag_14 (two weeks ago)\n",
      "  - lag_30 (last month)\n",
      "\n",
      "Sample with lag features:\n",
      "        date  transaction_volume  lag_1  lag_7\n",
      "0 2016-09-04                   1    NaN    NaN\n",
      "1 2016-09-05                   1    1.0    NaN\n",
      "2 2016-09-13                   1    1.0    NaN\n",
      "3 2016-09-15                   1    1.0    NaN\n",
      "4 2016-10-02                   1    1.0    NaN\n",
      "5 2016-10-03                   8    1.0    NaN\n",
      "6 2016-10-04                  63    8.0    NaN\n",
      "7 2016-10-05                  47   63.0    1.0\n",
      "8 2016-10-06                  51   47.0    1.0\n",
      "9 2016-10-07                  46   51.0    1.0\n",
      "\n",
      "Note: First rows have NaN for lags (no historical data yet)\n"
     ]
    }
   ],
   "source": [
    "# 2. LAG FEATURES (historical values)\n",
    "print(\"\\n[2/5] Adding lag features...\")\n",
    "\n",
    "# Shift transaction volumes back in time\n",
    "df['lag_1'] = df['transaction_volume'].shift(1)   # Yesterday\n",
    "df['lag_7'] = df['transaction_volume'].shift(7)   # Last week same day\n",
    "df['lag_14'] = df['transaction_volume'].shift(14) # Two weeks ago\n",
    "df['lag_30'] = df['transaction_volume'].shift(30) # Last month\n",
    "\n",
    "print(f\"✓ Added 4 lag features\")\n",
    "print(f\"  - lag_1 (yesterday)\")\n",
    "print(f\"  - lag_7 (last week)\")\n",
    "print(f\"  - lag_14 (two weeks ago)\")\n",
    "print(f\"  - lag_30 (last month)\")\n",
    "\n",
    "print(f\"\\nSample with lag features:\")\n",
    "print(df[['date', 'transaction_volume', 'lag_1', 'lag_7']].head(10))\n",
    "print(f\"\\nNote: First rows have NaN for lags (no historical data yet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e507306e-781b-4449-bc23-3e6865509b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Adding rolling statistics...\n",
      "✓ Added 7 rolling statistics features\n",
      "\n",
      "[4/5] Adding Brazilian holidays...\n",
      "✓ Added 3 holiday features\n",
      "  - is_holiday, days_to_holiday, days_from_holiday\n",
      "\n",
      "[5/5] Adding trend features...\n",
      "✓ Added 3 trend features\n",
      "  - days_since_start, transaction_growth, momentum_7\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "Total features: 28\n",
      "Original: 2 columns → Enriched: 28 columns\n"
     ]
    }
   ],
   "source": [
    "# 3. ROLLING STATISTICS (moving averages and trends)\n",
    "print(\"\\n[3/5] Adding rolling statistics...\")\n",
    "\n",
    "df['rolling_mean_7'] = df['transaction_volume'].rolling(window=7).mean()\n",
    "df['rolling_mean_14'] = df['transaction_volume'].rolling(window=14).mean()\n",
    "df['rolling_mean_30'] = df['transaction_volume'].rolling(window=30).mean()\n",
    "df['rolling_std_7'] = df['transaction_volume'].rolling(window=7).std()\n",
    "df['rolling_std_30'] = df['transaction_volume'].rolling(window=30).std()\n",
    "df['rolling_min_7'] = df['transaction_volume'].rolling(window=7).min()\n",
    "df['rolling_max_7'] = df['transaction_volume'].rolling(window=7).max()\n",
    "\n",
    "print(f\"✓ Added 7 rolling statistics features\")\n",
    "\n",
    "# 4. BRAZILIAN HOLIDAYS\n",
    "print(\"\\n[4/5] Adding Brazilian holidays...\")\n",
    "\n",
    "# Major Brazilian holidays that affect e-commerce\n",
    "brazilian_holidays = [\n",
    "    '2016-01-01', '2016-04-21', '2016-05-01', '2016-09-07', '2016-10-12', '2016-11-02', '2016-11-15', '2016-11-25', '2016-12-25',\n",
    "    '2017-01-01', '2017-04-21', '2017-05-01', '2017-09-07', '2017-10-12', '2017-11-02', '2017-11-15', '2017-11-24', '2017-12-25',\n",
    "    '2018-01-01', '2018-04-21', '2018-05-01', '2018-09-07', '2018-10-12', '2018-11-02', '2018-11-15', '2018-11-23', '2018-12-25'\n",
    "]\n",
    "\n",
    "df['is_holiday'] = df['date'].astype(str).isin(brazilian_holidays).astype(int)\n",
    "\n",
    "# Days before/after holidays\n",
    "df['days_to_holiday'] = 0\n",
    "df['days_from_holiday'] = 0\n",
    "\n",
    "for holiday in pd.to_datetime(brazilian_holidays):\n",
    "    days_diff = (df['date'] - holiday).dt.days\n",
    "    df.loc[(days_diff < 0) & (days_diff > -7), 'days_to_holiday'] = abs(days_diff)\n",
    "    df.loc[(days_diff > 0) & (days_diff < 7), 'days_from_holiday'] = days_diff\n",
    "\n",
    "print(f\"✓ Added 3 holiday features\")\n",
    "print(f\"  - is_holiday, days_to_holiday, days_from_holiday\")\n",
    "\n",
    "# 5. TREND FEATURES\n",
    "print(\"\\n[5/5] Adding trend features...\")\n",
    "\n",
    "df['days_since_start'] = (df['date'] - df['date'].min()).dt.days\n",
    "df['transaction_growth'] = df['transaction_volume'].pct_change()\n",
    "df['momentum_7'] = df['transaction_volume'].diff(7)  # Change from last week\n",
    "\n",
    "print(f\"✓ Added 3 trend features\")\n",
    "print(f\"  - days_since_start, transaction_growth, momentum_7\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"Original: 2 columns → Enriched: {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b704c23c-8a15-420d-8d7a-ac216b21ab0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values check:\n",
      "lag_30                30\n",
      "rolling_mean_30       29\n",
      "rolling_std_30        29\n",
      "lag_14                14\n",
      "rolling_mean_14       13\n",
      "lag_7                  7\n",
      "momentum_7             7\n",
      "rolling_mean_7         6\n",
      "rolling_std_7          6\n",
      "rolling_min_7          6\n",
      "rolling_max_7          6\n",
      "lag_1                  1\n",
      "transaction_growth     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values check:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de895ea-1f5d-4bb4-af7e-414ac90a7c13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values check:\n",
      "lag_30                30\n",
      "rolling_mean_30       29\n",
      "rolling_std_30        29\n",
      "lag_14                14\n",
      "rolling_mean_14       13\n",
      "lag_7                  7\n",
      "momentum_7             7\n",
      "rolling_mean_7         6\n",
      "rolling_std_7          6\n",
      "rolling_min_7          6\n",
      "rolling_max_7          6\n",
      "lag_1                  1\n",
      "transaction_growth     1\n",
      "dtype: int64\n",
      "\n",
      "Rows with missing values: 30\n",
      "Filling missing values with forward fill method...\n",
      "✓ Missing values handled\n",
      "\n",
      "Final dataset shape: (610, 28)\n",
      "Columns: ['date', 'transaction_volume', 'day_of_week', 'day_name', 'month', 'quarter', 'day_of_month', 'week_of_year', 'is_weekend', 'is_month_start', 'is_month_end', 'lag_1', 'lag_7', 'lag_14', 'lag_30', 'rolling_mean_7', 'rolling_mean_14', 'rolling_mean_30', 'rolling_std_7', 'rolling_std_30', 'rolling_min_7', 'rolling_max_7', 'is_holiday', 'days_to_holiday', 'days_from_holiday', 'days_since_start', 'transaction_growth', 'momentum_7']\n",
      "\n",
      "✓ Saved enriched dataset to gs://transaction-forecast-data/processed_data/daily_volumes_enriched.csv\n",
      "\n",
      "============================================================\n",
      "READY FOR ADVANCED MODELING!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values check:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0].sort_values(ascending=False))\n",
    "\n",
    "print(f\"\\nRows with missing values: {df.isnull().any(axis=1).sum()}\")\n",
    "print(f\"Filling missing values with forward fill method...\")\n",
    "\n",
    "df_filled = df.ffill().fillna(0)  # Forward fill, then zero\n",
    "\n",
    "print(f\"✓ Missing values handled\")\n",
    "print(f\"\\nFinal dataset shape: {df_filled.shape}\")\n",
    "print(f\"Columns: {df_filled.columns.tolist()}\")\n",
    "\n",
    "# Save enriched dataset\n",
    "df_filled.to_csv(f'gs://{BUCKET_NAME}/processed_data/daily_volumes_enriched.csv', index=False)\n",
    "print(f\"\\n✓ Saved enriched dataset to gs://{BUCKET_NAME}/processed_data/daily_volumes_enriched.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY FOR ADVANCED MODELING!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed923e7-fd34-4eb2-b582-457f8c200bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
