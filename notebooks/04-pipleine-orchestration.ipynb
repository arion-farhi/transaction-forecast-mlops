{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a2994e31-1e6d-4f71-8dd0-d2ff6eb4e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Kubeflow Pipelines SDK installed\n"
     ]
    }
   ],
   "source": [
    "# 04_kubeflow_pipeline.ipynb\n",
    "# MLOps Pipeline with Kubeflow on Vertex AI\n",
    "\n",
    "\"\"\"\n",
    "This notebook creates a production ML pipeline that:\n",
    "1. Ingests data from GCS\n",
    "2. Engineers features\n",
    "3. Trains XGBoost model\n",
    "4. Evaluates against threshold\n",
    "5. Registers model to Vertex AI Model Registry\n",
    "6. Deploys if performance meets criteria\n",
    "\n",
    "Pipeline Architecture:\n",
    "┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n",
    "│   Ingest    │───▶│  Feature    │───▶│   Train     │\n",
    "│    Data     │    │ Engineering │    │   Model     │\n",
    "└─────────────┘    └─────────────┘    └─────────────┘\n",
    "                                            │\n",
    "                                            ▼\n",
    "┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n",
    "│   Deploy    │◀───│  Register   │◀───│  Evaluate   │\n",
    "│  (if pass)  │    │   Model     │    │   Model     │\n",
    "└─────────────┘    └─────────────┘    └─────────────┘\n",
    "\"\"\"\n",
    "\n",
    "# Install Kubeflow Pipelines SDK\n",
    "!pip install kfp google-cloud-aiplatform --quiet\n",
    "\n",
    "print(\"✓ Kubeflow Pipelines SDK installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0588b185-6d5b-4b2b-a63f-b663749c95aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: transaction-forecast-mlops\n",
      "Region: us-central1\n",
      "Bucket: gs://transaction-forecast-data\n",
      "\n",
      "✓ Vertex AI initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports and Pipeline Components\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Output, Input, Dataset, Model, Metrics\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI\n",
    "PROJECT_ID = \"transaction-forecast-mlops\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET = \"gs://transaction-forecast-data\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Bucket: {BUCKET}\")\n",
    "print(\"\\n✓ Vertex AI initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e905601e-53f6-44e3-89d1-6f5e57b981bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Data Ingestion Component\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"google-cloud-storage\"]\n",
    ")\n",
    "def ingest_data(\n",
    "    bucket_name: str,\n",
    "    output_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Load raw data from GCS and prepare for feature engineering.\"\"\"\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    # Download data\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    bucket.blob('processed_data/daily_volumes_clean.csv').download_to_filename('/tmp/data.csv')\n",
    "    \n",
    "    # Load and validate\n",
    "    df = pd.read_csv('/tmp/data.csv')\n",
    "    print(f\"✓ Loaded {len(df)} rows\")\n",
    "    print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    # Save to output path\n",
    "    df.to_csv(output_data.path, index=False)\n",
    "    print(f\"✓ Data saved to {output_data.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "680ec320-b3e5-4e2a-9e0b-56be2f5cd213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: Feature Engineering Component\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"holidays\"]\n",
    ")\n",
    "def engineer_features(\n",
    "    input_data: Input[Dataset],\n",
    "    output_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Create all features needed for XGBoost model.\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import holidays\n",
    "    \n",
    "    df = pd.read_csv(input_data.path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Temporal features\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['day_of_month'] = df['date'].dt.day\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 7, 14, 30]:\n",
    "        df[f'lag_{lag}'] = df['transaction_volume'].shift(lag)\n",
    "    \n",
    "    # Rolling features\n",
    "    for window in [7, 14, 30]:\n",
    "        df[f'rolling_mean_{window}'] = df['transaction_volume'].rolling(window).mean()\n",
    "    df['rolling_std_7'] = df['transaction_volume'].rolling(7).std()\n",
    "    df['rolling_std_30'] = df['transaction_volume'].rolling(30).std()\n",
    "    df['rolling_min_7'] = df['transaction_volume'].rolling(7).min()\n",
    "    df['rolling_max_7'] = df['transaction_volume'].rolling(7).max()\n",
    "    \n",
    "    # Holiday features\n",
    "    br_holidays = holidays.Brazil(years=[2016, 2017, 2018])\n",
    "    df['is_holiday'] = df['date'].dt.date.apply(lambda x: 1 if x in br_holidays else 0)\n",
    "    \n",
    "    # Trend features\n",
    "    df['days_since_start'] = (df['date'] - df['date'].min()).dt.days\n",
    "    df['transaction_growth'] = df['transaction_volume'].pct_change()\n",
    "    df['momentum_7'] = df['transaction_volume'].diff(7)\n",
    "    \n",
    "    # Fill NaN\n",
    "    df = df.fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    print(f\"✓ Created {len(df.columns)} features\")\n",
    "    df.to_csv(output_data.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e9ff9b2f-109d-4612-97c8-3070f5b0319e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Train component defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train Model Component\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"xgboost\", \"scikit-learn\", \"joblib\", \"google-cloud-storage\"]\n",
    ")\n",
    "def train_model(\n",
    "    input_data: Input[Dataset],\n",
    "    model_output: Output[Model],\n",
    "    metrics_output: Output[Metrics],\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"Train XGBoost model with best parameters.\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    \n",
    "    df = pd.read_csv(input_data.path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    exclude_cols = ['date', 'transaction_volume', 'day_name']\n",
    "    feature_cols = [c for c in df.columns if c not in exclude_cols and c in df.select_dtypes(include=[np.number]).columns]\n",
    "    \n",
    "    train = df.iloc[:-14]\n",
    "    test = df.iloc[-14:]\n",
    "    \n",
    "    X_train = train[feature_cols]\n",
    "    y_train = train['transaction_volume']\n",
    "    X_test = test[feature_cols]\n",
    "    y_test = test['transaction_volume']\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    \n",
    "    print(f\"✓ Model trained (random_state={random_state})\")\n",
    "    print(f\"  MAE:  {mae:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    metrics_output.log_metric(\"mae\", mae)\n",
    "    metrics_output.log_metric(\"rmse\", rmse)\n",
    "    metrics_output.log_metric(\"mape\", mape)\n",
    "    \n",
    "    os.makedirs(model_output.path, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(model_output.path, \"model.joblib\"))\n",
    "    joblib.dump(feature_cols, os.path.join(model_output.path, \"features.joblib\"))\n",
    "    print(f\"✓ Model saved to {model_output.path}\")\n",
    "\n",
    "print(\"✓ Train component defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "825db5f0-a275-4114-8de3-0a00be52e7e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Evaluate Model Component\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    metrics: Input[Metrics],\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    model_name: str,\n",
    "    initial_threshold: float = 10.0,\n",
    "    max_threshold: float = 15.0,\n",
    "    min_improvement: float = 0.5\n",
    ") -> bool:\n",
    "    \"\"\"Compare challenger against champion model.\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    \n",
    "    challenger_mape = metrics.metadata.get(\"mape\", 100)\n",
    "    \n",
    "    print(f\"EVALUATION\")\n",
    "    print(f\"   Challenger MAPE: {challenger_mape:.2f}%\")\n",
    "    \n",
    "    if challenger_mape > max_threshold:\n",
    "        print(f\"   Exceeds max threshold ({max_threshold}%) - REJECTED\")\n",
    "        return False\n",
    "    \n",
    "    models = aiplatform.Model.list(filter=f'display_name=\"{model_name}\"')\n",
    "    \n",
    "    if not models:\n",
    "        passed = challenger_mape <= initial_threshold\n",
    "        print(f\"   No champion exists\")\n",
    "        print(f\"   Initial threshold: {initial_threshold}%\")\n",
    "        print(f\"   {'PASSED' if passed else 'FAILED'}\")\n",
    "        return passed\n",
    "    \n",
    "    champion = models[0]\n",
    "    champion_mape_str = champion.labels.get(\"mape\", str(initial_threshold).replace(\".\", \"_\"))\n",
    "    champion_mape = float(champion_mape_str.replace(\"_\", \".\"))\n",
    "    \n",
    "    print(f\"   Champion MAPE: {champion_mape:.2f}%\")\n",
    "    print(f\"   Required improvement: {min_improvement}%\")\n",
    "    \n",
    "    improvement = champion_mape - challenger_mape\n",
    "    passed = improvement >= min_improvement\n",
    "    \n",
    "    print(f\"   Actual improvement: {improvement:.2f}%\")\n",
    "    print(f\"   {'CHALLENGER WINS' if passed else 'CHAMPION DEFENDED'}\")\n",
    "    \n",
    "    return passed\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f3f70f6-c363-473a-82be-b6cd1cb982a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Register Model Component\n",
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"joblib\"]\n",
    ")\n",
    "def register_model(\n",
    "    model_input: Input[Model],\n",
    "    metrics: Input[Metrics],\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    model_name: str\n",
    ") -> str:\n",
    "    \"\"\"Register model to Vertex AI Model Registry with metrics as labels.\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    \n",
    "    mape = metrics.metadata.get(\"mape\", 0)\n",
    "    mae = metrics.metadata.get(\"mae\", 0)\n",
    "    \n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        artifact_uri=model_input.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-2:latest\",\n",
    "        labels={\n",
    "            \"mape\": str(round(mape, 2)).replace(\".\", \"_\"),\n",
    "            \"mae\": str(round(mae, 2)).replace(\".\", \"_\")\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Model registered\")\n",
    "    print(f\"  Name: {model_name}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    print(f\"  Resource: {model.resource_name}\")\n",
    "    \n",
    "    return model.resource_name\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a552a832-96a4-4fa8-beca-4343891d3e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_19371/2662587398.py:30: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(eval_task.output == True, name=\"check-performance\"):\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define the Pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"transaction-forecast-pipeline\",\n",
    "    description=\"End-to-end MLOps pipeline for transaction volume forecasting\"\n",
    ")\n",
    "def transaction_forecast_pipeline(\n",
    "    bucket_name: str = \"transaction-forecast-data\",\n",
    "    project_id: str = \"transaction-forecast-mlops\",\n",
    "    region: str = \"us-central1\",\n",
    "    model_name: str = \"transaction-forecast-xgboost\",\n",
    "    initial_threshold: float = 10.0,\n",
    "    max_threshold: float = 15.0,\n",
    "    min_improvement: float = 0.5,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    ingest_task = ingest_data(bucket_name=bucket_name)\n",
    "    feature_task = engineer_features(input_data=ingest_task.outputs[\"output_data\"])\n",
    "    train_task = train_model(input_data=feature_task.outputs[\"output_data\"], random_state=random_state)\n",
    "    \n",
    "    eval_task = evaluate_model(\n",
    "        metrics=train_task.outputs[\"metrics_output\"],\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        model_name=model_name,\n",
    "        initial_threshold=initial_threshold,\n",
    "        max_threshold=max_threshold,\n",
    "        min_improvement=min_improvement\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(eval_task.output == True, name=\"check-performance\"):\n",
    "        register_task = register_model(\n",
    "            model_input=train_task.outputs[\"model_output\"],\n",
    "            metrics=train_task.outputs[\"metrics_output\"],\n",
    "            project_id=project_id,\n",
    "            region=region,\n",
    "            model_name=model_name\n",
    "        )\n",
    "\n",
    "print(\"✓ Pipeline defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0641223-660c-4436-bf6b-e3e34835e1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline compiled to transaction_forecast_pipeline.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Compile Pipeline\n",
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=transaction_forecast_pipeline,\n",
    "    package_path=\"transaction_forecast_pipeline.json\"\n",
    ")\n",
    "\n",
    "print(\"✓ Pipeline compiled to transaction_forecast_pipeline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b8fe2bba-38f5-45a0-8270-5d5a38914ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting pipeline to Vertex AI...\n",
      "This will take 10-15 minutes to run.\n",
      "\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1023117266322/locations/us-central1/pipelineJobs/transaction-forecast-pipeline-20251209033042\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1023117266322/locations/us-central1/pipelineJobs/transaction-forecast-pipeline-20251209033042')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/transaction-forecast-pipeline-20251209033042?project=1023117266322\n",
      "✓ Pipeline submitted!\n",
      "  View at: https://console.cloud.google.com/vertex-ai/pipelines/runs?project=transaction-forecast-mlops\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Submit Pipeline\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET\n",
    ")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"transaction-forecast-run-7\",\n",
    "    template_path=\"transaction_forecast_pipeline.json\",\n",
    "    pipeline_root=f\"{BUCKET}/pipeline_root\",\n",
    "    enable_caching=False,\n",
    "    parameter_values={\n",
    "        \"bucket_name\": \"transaction-forecast-data\",\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"model_name\": \"transaction-forecast-xgboost\",\n",
    "        \"initial_threshold\": 10.0,\n",
    "        \"max_threshold\": 15.0,\n",
    "        \"min_improvement\": 0.5,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Submitting pipeline to Vertex AI...\")\n",
    "print(\"This will take 10-15 minutes to run.\\n\")\n",
    "job.submit()\n",
    "print(f\"✓ Pipeline submitted!\")\n",
    "print(f\"  View at: https://console.cloud.google.com/vertex-ai/pipelines/runs?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f4740-a1a4-4341-b6c3-4a2441c49fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m136",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m136"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
